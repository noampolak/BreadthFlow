# ğŸ”§ Notebook Troubleshooting Guide

## âœ… **Current Status: WORKING**

The notebook has been fixed and should now work properly. Here's what was resolved:

### ğŸ”§ **Issues Fixed:**

1. **âœ… Docker Network Connectivity**: Fixed service URLs to use internal Docker network names
2. **âœ… MLflow API**: Simplified to use health endpoint instead of complex API calls
3. **âœ… Seldon Core**: Simplified deployment section (service has worker timeout issues)
4. **âœ… Feature Modules**: All working correctly with TA-Lib installed

### ğŸ¯ **What's Working:**

- âœ… **Data Pipeline**: http://data-pipeline:8001
- âœ… **Feature Engineering**: http://feature-engineering:8002
- âœ… **Model Training**: http://model-training:8003
- âœ… **AutoML**: http://automl:8004
- âœ… **MLflow**: http://mlflow:5000 (health check)
- âœ… **Feature Modules**: All 5 modules working (Technical, Financial, Microstructure, Time, Utils)

### âœ… **All Services Working:**

- **Seldon Core**: âœ… Fixed and working (port 8005)
- **Model Registry**: Available (port 8006)

## ğŸš€ **How to Use the Notebook:**

1. **Open Jupyter Lab**: http://localhost:8888 (Token: `breadthflow123`)
2. **Open the notebook**: `notebooks/multi_source_training_example.ipynb`
3. **Run all cells**: The connectivity test should now show âœ… for all core services
4. **Complete the workflow**: Generate features, train models, track with MLflow

## ğŸ”§ **If You Still Have Issues:**

### **Config File Not Found Error:**
**Error**: `FileNotFoundError: [Errno 2] No such file or directory: '../experiments/multi_source_analysis/config.yaml'`

**Solution**: The config file path is incorrect. It should be `experiments/multi_source_analysis/config.yaml` (without `../`)

**Fix**: Update the notebook cell to use the correct path:
```python
config_path = 'experiments/multi_source_analysis/config.yaml'  # Correct path
```

### **Circular Reference Error:**
**Error**: `NameError: name 'feature_categories' is not defined`

**Solution**: The dictionary comprehension is trying to reference itself before it's fully created.

**Fix**: Replace the circular reference with a two-step approach:
```python
# Step 1: Create the dictionary without 'Other'
feature_categories = {
    'Technical': [col for col in X.columns if col in ['rsi', 'macd', 'bb_', 'stoch_']],
    'Microstructure': [col for col in X.columns if col in ['volume_', 'pvt', 'obv', 'atr', 'volatility']],
    'Time': [col for col in X.columns if col in ['hour', 'day_', 'month', 'quarter', 'year', 'is_', 'season']]
}

# Step 2: Calculate 'Other' category after dictionary is created
all_categorized = []
for cat in ['Technical', 'Microstructure', 'Time']:
    all_categorized.extend(feature_categories[cat])

feature_categories['Other'] = [col for col in X.columns if col not in ['symbol'] and col not in all_categorized]
```

### **Check Service Status:**
```bash
docker-compose -f docker-compose.ml.yml ps
```

### **Test Connectivity from Jupyter:**
```python
import requests
import os

# Test core services
services = {
    'Data Pipeline': 'http://data-pipeline:8001/health',
    'Feature Engineering': 'http://feature-engineering:8002/health', 
    'Model Training': 'http://model-training:8003/health',
    'AutoML': 'http://automl:8004/health'
}

for service_name, url in services.items():
    try:
        response = requests.get(url, timeout=5)
        print(f'âœ… {service_name}: {response.status_code}')
    except Exception as e:
        print(f'âŒ {service_name}: {e}')
```

### **Restart Services if Needed:**
```bash
# Restart all services
docker-compose -f docker-compose.ml.yml restart

# Or restart specific service
docker-compose -f docker-compose.ml.yml restart jupyter
```

## ğŸ“Š **Expected Workflow:**

1. **âœ… Service Connectivity Test**: Should show 4/4 services connected
2. **âœ… Configuration Loading**: Load experiment config
3. **âœ… Sample Data Generation**: Create realistic OHLCV data
4. **âœ… Feature Generation**: Generate 51+ features using generic modules
5. **âœ… Data Visualization**: Show feature distributions and correlations
6. **âœ… Model Training**: Train models using ML pipeline
7. **âœ… MLflow Integration**: Track experiments
8. **âœ… Model Deployment**: Deploy and test models with Seldon Core

## ğŸ‰ **Success Indicators:**

- All service connectivity tests show âœ…
- Feature generation produces 50+ features
- Model training completes successfully
- MLflow shows experiment tracking

**The notebook is now ready to use! ğŸš€**
