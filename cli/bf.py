#!/usr/bin/env python3
"""
Breadth/Thrust Signals POC - Command Line Interface

A comprehensive CLI for managing the entire system using Python and Click.
"""

import click
import subprocess
import time
import requests
import json
import os
import sys
import logging
from pathlib import Path
from typing import List, Optional
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import the new Spark job submitter
from cli.spark_job_submitter import SparkJobSubmitter


def create_spark_session(app_name: str):
    """
    Create a Spark session that connects to the Docker Spark cluster.
    
    Args:
        app_name: Name of the Spark application
        
    Returns:
        Configured SparkSession
    """
    from pyspark.sql import SparkSession
    import socket
    
    # Get host machine's IP address for executors to connect to driver
    # Use the actual network interface IP, not hostname resolution
    import subprocess
    try:
        # Get the actual network IP that Docker containers can reach
        result = subprocess.run(['ifconfig', 'en0'], capture_output=True, text=True, check=True)
        for line in result.stdout.split('\n'):
            if 'inet ' in line:
                host_ip = line.strip().split()[1]
                break
        else:
            # Fallback to hostname resolution
            host_ip = socket.gethostbyname(socket.gethostname())
    except:
        # Fallback to hostname resolution
        host_ip = socket.gethostbyname(socket.gethostname())
    
    # We're on host, connect to Docker Spark cluster with Delta Lake and S3 support
    return SparkSession.builder \
        .appName(app_name) \
        .master("spark://localhost:7077") \
        .config("spark.driver.host", host_ip) \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.executor.memory", "512m") \
        .config("spark.executor.cores", "1") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memoryOverhead", "256m") \
        .config("spark.driver.memoryOverhead", "256m") \
        .config("spark.memory.fraction", "0.8") \
        .config("spark.memory.storageFraction", "0.3") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.submit.deployMode", "cluster") \
        .config("spark.driver.extraClassPath", "/opt/bitnami/spark/jars/*") \
        .config("spark.executor.extraClassPath", "/opt/bitnami/spark/jars/*") \
        .config("spark.network.timeout", "800s") \
        .config("spark.executor.heartbeatInterval", "60s") \
        .config("spark.storage.blockManagerSlaveTimeoutMs", "800000") \
        .config("spark.rpc.askTimeout", "800s") \
        .config("spark.rpc.lookupTimeout", "800s") \
        .config("spark.dynamicAllocation.enabled", "false") \
        .config("spark.speculation", "false") \
        .config("spark.pyspark.python", "python3") \
        .config("spark.pyspark.driver.python", "python3") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
        .config("spark.sql.adaptive.enabled", "false") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "false") \
        .config("spark.sql.adaptive.skewJoin.enabled", "false") \
        .getOrCreate()


@click.group()
@click.version_option(version="1.0.0")
def cli():
    """
    Breadth/Thrust Signals POC - Real-time market breadth analysis system.
    
    Built with PySpark, Kafka, Delta Lake, and modern big data technologies.
    """
    pass


# ============================================================================
# Infrastructure Management Commands
# ============================================================================

@cli.group()
def infra():
    """Infrastructure management commands."""
    pass


@infra.command()
@click.option('--wait', default=30, help='Seconds to wait for services to start')
def start(wait):
    """Start all infrastructure services."""
    click.echo("üöÄ Starting Breadth/Thrust Signals Infrastructure")
    
    # Check if Docker Compose file exists
    docker_compose_file = Path("infra/docker-compose.yml")
    if not docker_compose_file.exists():
        click.echo("‚ùå Docker Compose file not found!")
        click.echo("Please ensure infra/docker-compose.yml exists")
        return
    
    try:
        # Start services
        click.echo("üì¶ Starting Docker services...")
        subprocess.run(
            ["docker", "compose", "-f", "infra/docker-compose.yml", "up", "-d"],
            check=True
        )
        click.echo("‚úÖ Docker services started successfully")
        
        # Wait for services
        if wait > 0:
            click.echo(f"‚è≥ Waiting {wait} seconds for services to start...")
            time.sleep(wait)
        
        # Health check
        click.echo("üè• Performing health checks...")
        health_status = check_infrastructure_health()
        
        if health_status['all_healthy']:
            click.echo("üéâ All services are healthy! Infrastructure is ready.")
            show_service_urls()
        else:
            click.echo(f"‚ö†Ô∏è  {health_status['healthy_count']}/{health_status['total_count']} services are healthy")
            click.echo("Check logs with: bf infra logs")
            
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Failed to start Docker services: {e}")
    except Exception as e:
        click.echo(f"‚ùå Unexpected error: {e}")


@infra.command()
def stop():
    """Stop all infrastructure services."""
    click.echo("üõë Stopping infrastructure services...")
    try:
        subprocess.run(
            ["docker", "compose", "-f", "infra/docker-compose.yml", "down"],
            check=True
        )
        click.echo("‚úÖ Infrastructure stopped successfully")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Failed to stop services: {e}")


@infra.command()
def restart():
    """Restart all infrastructure services."""
    click.echo("üîÑ Restarting infrastructure services...")
    try:
        # Stop
        subprocess.run(
            ["docker", "compose", "-f", "infra/docker-compose.yml", "down"],
            check=True
        )
        click.echo("‚úÖ Services stopped")
        
        # Start
        subprocess.run(
            ["docker", "compose", "-f", "infra/docker-compose.yml", "up", "-d"],
            check=True
        )
        click.echo("‚úÖ Services started")
        
        # Health check
        time.sleep(30)
        health_status = check_infrastructure_health()
        if health_status['all_healthy']:
            click.echo("üéâ All services are healthy!")
        else:
            click.echo(f"‚ö†Ô∏è  {health_status['healthy_count']}/{health_status['total_count']} services are healthy")
            
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Failed to restart services: {e}")


@infra.command()
def status():
    """Check status of infrastructure services."""
    click.echo("üìä Infrastructure Status")
    click.echo("=" * 40)
    
    # Show service URLs
    show_service_urls()
    click.echo()
    
    # Check health
    health_status = check_infrastructure_health()
    click.echo(f"Health: {health_status['healthy_count']}/{health_status['total_count']} services healthy")
    
    # Show Docker status
    try:
        result = subprocess.run(
            ["docker", "compose", "-f", "infra/docker-compose.yml", "ps"],
            capture_output=True, text=True, check=True
        )
        click.echo("\nDocker Service Status:")
        click.echo(result.stdout)
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Failed to get Docker status: {e}")


@infra.command()
@click.option('--follow', '-f', is_flag=True, help='Follow logs')
def logs(follow):
    """Show infrastructure service logs."""
    try:
        cmd = ["docker", "compose", "-f", "infra/docker-compose.yml", "logs"]
        if follow:
            cmd.append("-f")
        subprocess.run(cmd)
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Failed to show logs: {e}")


@infra.command()
def health():
    """Perform detailed health checks."""
    click.echo("üè• Performing detailed health checks...")
    health_status = check_infrastructure_health()
    
    if health_status['all_healthy']:
        click.echo("üéâ All services are healthy!")
    else:
        click.echo(f"‚ö†Ô∏è  {health_status['healthy_count']}/{health_status['total_count']} services are healthy")
        for service, status in health_status['services'].items():
            icon = "‚úÖ" if status else "‚ùå"
            click.echo(f"  {icon} {service}")


# ============================================================================
# Data Management Commands
# ============================================================================

@cli.group()
def data():
    """Data management commands."""
    pass


@data.command()
def summary():
    """Show summary of stored data."""
    click.echo("üìä Data Summary")
    
    try:
        # Try to use local PySpark first
        try:
            # Import here to avoid circular imports
            from ingestion.data_fetcher import create_data_fetcher
            
            # Create Spark session - connect to Docker cluster
            spark = create_spark_session("DataSummary")
            
            # Create data fetcher
            fetcher = create_data_fetcher(spark)
            
            # Get summary
            summary = fetcher.get_data_summary()
            
            if "error" in summary:
                click.echo(f"‚ùå Error: {summary['error']}")
                return
            
            click.echo(f"üìä Total records: {summary['total_records']:,}")
            click.echo(f"üìà Unique symbols: {summary['unique_symbols']}")
            click.echo(f"üìÖ Date range: {summary['date_range']['start']} to {summary['date_range']['end']}")
            click.echo(f"‚≠ê Average quality score: {summary['avg_quality_score']:.3f}")
            
            if summary['symbols']:
                click.echo(f"üìã Symbols: {', '.join(summary['symbols'][:10])}")
                if len(summary['symbols']) > 10:
                    click.echo(f"   ... and {len(summary['symbols']) - 10} more")
            
            spark.stop()
            
        except Exception as local_error:
            click.echo(f"‚ùå PySpark error: {str(local_error)}")
            click.echo("üí° Make sure your Spark cluster is running: docker-compose up -d")
            return
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")


@data.command()
@click.option('--symbols', help='Comma-separated symbols')
@click.option('--symbol-list', help='Use predefined symbol list (e.g., demo_small, sp500, tech_leaders)')
@click.option('--start-date', default='2023-01-01', help='Start date (YYYY-MM-DD)')
@click.option('--end-date', default='2024-12-31', help='End date (YYYY-MM-DD)')
@click.option('--parallel', default=2, help='Number of parallel workers')
@click.option('--table-path', help='Delta table path for storage')
def fetch(symbols, symbol_list, start_date, end_date, parallel, table_path):
    """Fetch historical data for symbols using proper Spark infrastructure."""
    
    # Handle symbol selection
    if symbol_list:
        try:
            from features.common.symbols import get_symbol_manager
            manager = get_symbol_manager()
            symbols_to_fetch = manager.get_symbol_list(symbol_list)
            click.echo(f"üì• Fetching data for symbol list: {symbol_list}")
            click.echo(f"üìä Symbols: {', '.join(symbols_to_fetch[:5])}{'...' if len(symbols_to_fetch) > 5 else ''}")
        except Exception as e:
            click.echo(f"‚ùå Error loading symbol list '{symbol_list}': {e}")
            return
    elif symbols:
        symbols_to_fetch = [s.strip() for s in symbols.split(',')]
        click.echo(f"üì• Fetching data for custom symbols: {symbols}")
    else:
        # Default to demo symbols
        from features.common.symbols import get_demo_symbols
        symbols_to_fetch = get_demo_symbols("small")
        click.echo(f"üì• Fetching data for default demo symbols: {', '.join(symbols_to_fetch)}")
    
    click.echo(f"üìÖ Period: {start_date} to {end_date}")
    click.echo(f"‚ö° Parallel workers: {parallel}")
    click.echo(f"üìä Total symbols: {len(symbols_to_fetch)}")
    
    if table_path:
        click.echo(f"üíæ Storage: {table_path}")
    else:
        click.echo("üíæ Storage: MinIO (s3://breadthflow/ohlcv/)")
    
    try:
        # Use the new Spark job submitter for proper infrastructure
        click.echo("üîÑ Initializing Spark job submitter...")
        submitter = SparkJobSubmitter()
        
        # Check cluster status
        click.echo("üîç Checking Spark cluster status...")
        if not submitter.check_cluster_status():
            click.echo("‚ùå Spark cluster is not healthy. Please check infrastructure status.")
            return
        
        worker_count = submitter.get_worker_count()
        click.echo(f"‚úÖ Spark cluster is healthy with {worker_count} workers")
        
        # Limit parallel workers to available Spark workers
        actual_workers = min(parallel, worker_count)
        if actual_workers != parallel:
            click.echo(f"‚ö†Ô∏è  Limiting parallel workers to {actual_workers} (available Spark workers)")
        
        # Submit the job to the cluster
        click.echo("üöÄ Submitting data fetch job to Spark cluster...")
        result = submitter.submit_data_fetch_job(
            symbols=symbols_to_fetch,
            start_date=start_date,
            end_date=end_date,
            max_workers=actual_workers
        )
        
        click.echo("‚úÖ Job submitted successfully to Spark cluster")
        click.echo("üìä Check Spark UI at http://localhost:8080 for job progress")
        click.echo("üìä Check Spark History at http://localhost:18080 for completed jobs")
        
    except Exception as e:
        click.echo(f"‚ùå Error submitting job: {str(e)}")
        click.echo("üîç Please check that the Spark cluster is running: poetry run bf infra status")


@data.command()
def symbols():
    """List available symbol lists and their information."""
    click.echo("üìä Available Symbol Lists")
    click.echo("=" * 40)
    
    try:
        from features.common.symbols import get_symbol_manager
        manager = get_symbol_manager()
        
        # Get summary
        summary = manager.get_symbol_list_summary()
        
        click.echo(f"üìã Total Lists: {summary['total_lists']}")
        click.echo(f"üìù Description: {summary['metadata']['description']}")
        click.echo(f"üïí Last Updated: {summary['metadata']['last_updated']}")
        
        click.echo("\nüìä Symbol Lists:")
        for list_name, list_info in summary['lists'].items():
            click.echo(f"  ‚Ä¢ {list_name}: {list_info['name']}")
            click.echo(f"    Description: {list_info['description']}")
            click.echo(f"    Symbols: {list_info['symbol_count']}")
            if list_info['symbol_count'] <= 10:
                click.echo(f"    List: {', '.join(list_info['symbols'])}")
            else:
                click.echo(f"    Sample: {', '.join(list_info['symbols'][:5])}...")
            click.echo()
        
        click.echo("üí° Recommendations:")
        for use_case, list_name in summary['recommendations'].items():
            click.echo(f"  ‚Ä¢ {use_case}: {list_name}")
        
        click.echo("\nüîß Usage Examples:")
        click.echo("  ‚Ä¢ poetry run bf data fetch --symbol-list demo_small")
        click.echo("  ‚Ä¢ poetry run bf data fetch --symbol-list sp500")
        click.echo("  ‚Ä¢ poetry run bf data fetch --symbol-list tech_leaders")
        click.echo("  ‚Ä¢ poetry run bf data fetch --symbols AAPL,MSFT,GOOGL")
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")


@data.command()
@click.option('--speed', default=60, help='Replay speed multiplier')
@click.option('--start-date', help='Start date (YYYY-MM-DD)')
@click.option('--end-date', help='End date (YYYY-MM-DD)')
@click.option('--symbols', help='Comma-separated symbols to replay')
@click.option('--topic', default='quotes_replay', help='Kafka topic')
@click.option('--table-path', help='Delta table path')
def replay(speed, start_date, end_date, symbols, topic, table_path):
    """Start data replay from Delta to Kafka."""
    click.echo(f"üîÑ Starting data replay")
    click.echo(f"‚ö° Speed: {speed}x")
    click.echo(f"üì° Topic: {topic}")
    
    if start_date:
        click.echo(f"üìÖ Start date: {start_date}")
    if end_date:
        click.echo(f"üìÖ End date: {end_date}")
    if symbols:
        click.echo(f"üìä Symbols: {symbols}")
    
    try:
        # Import here to avoid circular imports
        from ingestion.replay import create_replay_manager, ReplayConfig
        
        # Create Spark session - connect to Docker cluster
        spark = create_spark_session("ReplayManager")
        
        # Create replay manager
        replay_manager = create_replay_manager(spark)
        
        # Parse symbols if provided
        symbol_list = None
        if symbols:
            symbol_list = [s.strip() for s in symbols.split(',')]
        
        # Set table path
        if table_path is None:
            table_path = "data/ohlcv"
        
        # Load historical data
        click.echo("üìÇ Loading historical data...")
        df = replay_manager.load_historical_data(
            table_path=table_path,
            start_date=start_date,
            end_date=end_date,
            symbols=symbol_list
        )
        
        if df.count() == 0:
            click.echo("‚ùå No data found for replay")
            return
        
        # Configure replay
        config = ReplayConfig(
            speed_multiplier=float(speed),
            topic_name=topic,
            start_date=start_date,
            end_date=end_date,
            symbols=symbol_list
        )
        
        # Start replay
        click.echo("üöÄ Starting replay...")
        result = replay_manager.replay_data(df, config)
        
        if result["success"]:
            click.echo(f"‚úÖ Replay completed successfully")
            click.echo(f"üìä Records processed: {result['processed_records']}")
            click.echo(f"‚è±Ô∏è  Duration: {result['duration_seconds']:.2f}s")
            click.echo(f"‚ö° Effective speed: {result['effective_speed']:.1f} records/sec")
        else:
            click.echo(f"‚ùå Replay failed: {result.get('error', 'Unknown error')}")
        
        # Cleanup
        replay_manager.cleanup()
        spark.stop()
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")
        click.echo("üí° Make sure infrastructure is running: poetry run bf infra start")


# ============================================================================
# Signal Generation Commands
# ============================================================================

@cli.group()
def signals():
    """Signal generation commands."""
    pass


@signals.command()
@click.option('--date', help='Date to generate signals for (YYYY-MM-DD)')
@click.option('--start-date', help='Start date for signal generation (YYYY-MM-DD)')
@click.option('--end-date', help='End date for signal generation (YYYY-MM-DD)')
@click.option('--symbols', help='Comma-separated symbols to analyze')
@click.option('--symbol-list', help='Use predefined symbol list (e.g., demo_small, sp500, tech_leaders)')
@click.option('--force-recalculate', is_flag=True, help='Force recalculation of all features')
def generate(date, start_date, end_date, symbols, symbol_list, force_recalculate):
    """Generate trading signals."""
    click.echo("üéØ Generating trading signals")
    
    if date:
        click.echo(f"üìÖ Date: {date}")
    elif start_date and end_date:
        click.echo(f"üìÖ Period: {start_date} to {end_date}")
    else:
        click.echo("üìÖ Date: Latest available")
    
    # Handle symbol selection
    if symbol_list:
        try:
            from features.common.symbols import get_symbol_manager
            manager = get_symbol_manager()
            symbols_to_analyze = manager.get_symbol_list(symbol_list)
            click.echo(f"üìä Using symbol list: {symbol_list}")
            click.echo(f"üìà Symbols: {', '.join(symbols_to_analyze[:5])}{'...' if len(symbols_to_analyze) > 5 else ''}")
        except Exception as e:
            click.echo(f"‚ùå Error loading symbol list '{symbol_list}': {e}")
            return
    elif symbols:
        symbols_to_analyze = [s.strip() for s in symbols.split(',')]
        click.echo(f"üìä Custom symbols: {symbols}")
    else:
        symbols_to_analyze = None
        click.echo("üìä Using all available symbols")
    
    if force_recalculate:
        click.echo("üîÑ Force recalculation enabled")
    
    try:
        # Import here to avoid circular imports
        from model.signal_generator import create_signal_generator
        
        # Create Spark session - connect to Docker cluster
        spark = create_spark_session("SignalGenerator")
        
        # Create signal generator
        generator = create_signal_generator(spark)
        
        # Generate signals
        if date:
            # Generate for specific date
            result = generator.generate_signals_for_date(date, symbols_to_analyze)
        else:
            # Generate for period
            result = generator.generate_signals(
                symbols=symbols_to_analyze,
                start_date=start_date,
                end_date=end_date,
                force_recalculate=force_recalculate
            )
        
        if result["success"]:
            click.echo("‚úÖ Signal generation completed successfully")
            
            if "generation_time_seconds" in result:
                click.echo(f"‚è±Ô∏è  Generation time: {result['generation_time_seconds']:.2f}s")
            
            if "total_signals_generated" in result:
                click.echo(f"üìä Total signals: {result['total_signals_generated']}")
                click.echo(f"üü¢ Buy signals: {result['buy_signals']}")
                click.echo(f"üî¥ Sell signals: {result['sell_signals']}")
                click.echo(f"‚≠ê Strong signals: {result['strong_signals']}")
            
            if "composite_score" in result:
                click.echo(f"üìà Composite score: {result['composite_score']:.2f}")
                click.echo(f"üéØ Signal direction: {result['signal_direction']}")
                click.echo(f"üí™ Signal strength: {result['signal_strength']}")
                click.echo(f"üéØ Confidence: {result['confidence_score']:.2f}")
        else:
            click.echo(f"‚ùå Signal generation failed: {result.get('error', 'Unknown error')}")
        
        spark.stop()
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")
        click.echo("üí° Make sure infrastructure is running: poetry run bf infra start")


@signals.command()
def summary():
    """Show summary of generated signals."""
    click.echo("üìä Signal Summary")
    
    try:
        # Import here to avoid circular imports
        from model.signal_generator import create_signal_generator
        
        # Create Spark session - connect to Docker cluster
        spark = create_spark_session("SignalSummary")
        
        # Create signal generator
        generator = create_signal_generator(spark)
        
        # Get summary
        summary = generator.get_signals_summary()
        
        if "error" in summary:
            click.echo(f"‚ùå Error: {summary['error']}")
            return
        
        click.echo(f"üìä Total signals: {summary['total_signals']:,}")
        click.echo(f"üìà Average score: {summary['avg_score']:.2f}")
        click.echo(f"üéØ Average confidence: {summary['avg_confidence']:.2f}")
        click.echo(f"üü¢ Buy signals: {summary['buy_signals']}")
        click.echo(f"üî¥ Sell signals: {summary['sell_signals']}")
        click.echo(f"‚≠ê Strong buy signals: {summary['strong_buy_signals']}")
        click.echo(f"‚≠ê Strong sell signals: {summary['strong_sell_signals']}")
        click.echo(f"üìà Bullish days: {summary['bullish_days']}")
        click.echo(f"üìâ Bearish days: {summary['bearish_days']}")
        click.echo(f"‚û°Ô∏è  Neutral days: {summary['neutral_days']}")
        
        spark.stop()
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")


@signals.command()
@click.option('--query', required=True, help='Search query')
@click.option('--index', default='breadth_signals', help='Elasticsearch index')
def search(query, index):
    """Search signals using Elasticsearch."""
    click.echo(f"üîç Searching signals")
    click.echo(f"üìù Query: {query}")
    click.echo(f"üìä Index: {index}")
    
    # This will be implemented in the search module
    click.echo("üîÑ Signal search not yet implemented")
    click.echo("Run: python -m streaming.elasticsearch_sink")


# ============================================================================
# Backtesting Commands
# ============================================================================

@cli.group()
def backtest():
    """Backtesting commands."""
    pass


@backtest.command()
@click.option('--from-date', required=True, help='Start date (YYYY-MM-DD)')
@click.option('--to-date', required=True, help='End date (YYYY-MM-DD)')
@click.option('--symbols', help='Comma-separated symbols to trade')
@click.option('--symbol-list', help='Use predefined symbol list (e.g., demo_small, sp500, tech_leaders)')
@click.option('--initial-capital', default=100000.0, help='Initial capital amount')
@click.option('--position-size', default=0.1, help='Position size as percentage of capital')
@click.option('--max-positions', default=10, help='Maximum number of concurrent positions')
@click.option('--commission-rate', default=0.001, help='Commission rate (e.g., 0.001 for 0.1%)')
@click.option('--slippage-rate', default=0.0005, help='Slippage rate (e.g., 0.0005 for 0.05%)')
@click.option('--save-results', is_flag=True, help='Save results to Delta Lake')
def run(from_date, to_date, symbols, symbol_list, initial_capital, position_size, max_positions, commission_rate, slippage_rate, save_results):
    """Run backtesting analysis."""
    click.echo(f"üìä Running backtest")
    click.echo(f"üìÖ Period: {from_date} to {to_date}")
    click.echo(f"üí∞ Initial capital: ${initial_capital:,.2f}")
    click.echo(f"üìà Position size: {position_size:.1%}")
    click.echo(f"üî¢ Max positions: {max_positions}")
    click.echo(f"üí∏ Commission rate: {commission_rate:.3%}")
    click.echo(f"üìâ Slippage rate: {slippage_rate:.3%}")
    
    # Handle symbol selection
    if symbol_list:
        try:
            from features.common.symbols import get_symbol_manager
            manager = get_symbol_manager()
            symbols_to_trade = manager.get_symbol_list(symbol_list)
            click.echo(f"üìä Using symbol list: {symbol_list}")
            click.echo(f"üìà Symbols: {', '.join(symbols_to_trade[:5])}{'...' if len(symbols_to_trade) > 5 else ''}")
        except Exception as e:
            click.echo(f"‚ùå Error loading symbol list '{symbol_list}': {e}")
            return
    elif symbols:
        symbols_to_trade = [s.strip() for s in symbols.split(',')]
        click.echo(f"üìä Custom symbols: {symbols}")
    else:
        symbols_to_trade = None
        click.echo("üìä Using all available symbols")
    
    try:
        # Import here to avoid circular imports
        from backtests.engine import create_backtest_engine, BacktestConfig
        
        # Create Spark session - connect to Docker cluster
        spark = create_spark_session("BacktestEngine")
        
        # Create backtest configuration
        config = BacktestConfig(
            initial_capital=initial_capital,
            position_size_pct=position_size,
            max_positions=max_positions,
            commission_rate=commission_rate,
            slippage_rate=slippage_rate
        )
        
        # Create backtest engine
        engine = create_backtest_engine(spark, config)
        
        # Run backtest
        results = engine.run_backtest(
            start_date=from_date,
            end_date=to_date,
            symbols=symbols_to_trade,
            save_results=save_results
        )
        
        if results:
            click.echo("‚úÖ Backtest completed successfully")
            click.echo(f"üìä Total Return: {results.total_return:.2%}")
            click.echo(f"üìà Annualized Return: {results.annualized_return:.2%}")
            click.echo(f"üìä Sharpe Ratio: {results.sharpe_ratio:.2f}")
            click.echo(f"üìâ Max Drawdown: {results.max_drawdown:.2%}")
            click.echo(f"üéØ Hit Rate: {results.hit_rate:.2%}")
            click.echo(f"üí∞ Total Trades: {results.total_trades}")
            click.echo(f"üìä Winning Trades: {results.winning_trades}")
            click.echo(f"üìâ Losing Trades: {results.losing_trades}")
            click.echo(f"üíµ Average Win: ${results.avg_win:,.2f}")
            click.echo(f"üí∏ Average Loss: ${results.avg_loss:,.2f}")
            click.echo(f"üìà Profit Factor: {results.profit_factor:.2f}")
            click.echo(f"üéØ Signal Accuracy: {results.signal_accuracy:.2%}")
            
            if save_results:
                click.echo(f"üíæ Results saved to backtests/out/results_{from_date}_{to_date}")
        else:
            click.echo("‚ùå Backtest failed")
        
        spark.stop()
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")
        click.echo("üí° Make sure infrastructure is running: poetry run bf infra start")


@backtest.command()
@click.option('--results-path', help='Path to backtest results in Delta Lake')
@click.option('--start-date', help='Start date filter for analysis')
@click.option('--end-date', help='End date filter for analysis')
def analyze(results_path, start_date, end_date):
    """Analyze backtest results."""
    click.echo("üìà Analyzing backtest results")
    
    if results_path:
        click.echo(f"üìÑ Results path: {results_path}")
    else:
        click.echo("üìÑ Using latest backtest results")
    
    if start_date and end_date:
        click.echo(f"üìÖ Period: {start_date} to {end_date}")
    
    try:
        # Import here to avoid circular imports
        from backtests.metrics import create_performance_metrics
        from features.common.io import read_delta
        
        # Create Spark session - connect to Docker cluster
        spark = create_spark_session("BacktestAnalysis")
        
        # Load backtest results
        if results_path:
            results_df = read_delta(spark, results_path)
        else:
            # Try to find latest results
            import glob
            import os
            result_files = glob.glob("backtests/out/results_*")
            if result_files:
                latest_result = max(result_files, key=os.path.getctime)
                results_df = read_delta(spark, latest_result)
                click.echo(f"üìÑ Found latest results: {latest_result}")
            else:
                click.echo("‚ùå No backtest results found")
                return
        
        # Apply date filters if provided
        if start_date:
            results_df = results_df.filter(col("date") >= start_date)
        if end_date:
            results_df = results_df.filter(col("date") <= end_date)
        
        # Convert to pandas for analysis
        results_pdf = results_df.toPandas()
        
        if results_pdf.empty:
            click.echo("‚ùå No results found for the specified criteria")
            return
        
        # Calculate performance metrics
        metrics_calc = create_performance_metrics()
        
        # Extract returns from portfolio values
        portfolio_values = results_pdf["portfolio_value"].tolist()
        returns = []
        for i in range(1, len(portfolio_values)):
            daily_return = (portfolio_values[i] - portfolio_values[i-1]) / portfolio_values[i-1]
            returns.append(daily_return)
        
        # Calculate comprehensive metrics
        metrics = metrics_calc.calculate_all_metrics(
            returns=returns,
            portfolio_values=portfolio_values
        )
        
        # Generate and display report
        report = metrics_calc.generate_performance_report(
            metrics, 
            "Breadth/Thrust Strategy Backtest"
        )
        click.echo(report)
        
        # Additional insights
        click.echo("üîç Additional Insights:")
        click.echo(f"üìä Total Days: {len(results_pdf)}")
        click.echo(f"üìà Final Portfolio Value: ${portfolio_values[-1]:,.2f}")
        click.echo(f"üí∞ Peak Portfolio Value: ${max(portfolio_values):,.2f}")
        click.echo(f"üìâ Trough Portfolio Value: ${min(portfolio_values):,.2f}")
        
        # Signal analysis
        if "signals_processed" in results_pdf.columns:
            avg_signals = results_pdf["signals_processed"].mean()
            avg_acted = results_pdf["signals_acted_upon"].mean()
            click.echo(f"üéØ Average Signals per Day: {avg_signals:.1f}")
            click.echo(f"‚ö° Average Signals Acted Upon: {avg_acted:.1f}")
        
        spark.stop()
        
    except Exception as e:
        click.echo(f"‚ùå Error: {str(e)}")
        click.echo("üí° Make sure backtest results exist: poetry run bf backtest run")


# ============================================================================
# Development Commands
# ============================================================================

@cli.group()
def dev():
    """Development commands."""
    pass


@dev.command()
def install():
    """Install Python dependencies using Poetry."""
    click.echo("üì¶ Installing Python dependencies with Poetry...")
    try:
        subprocess.run(["poetry", "install"], check=True)
        click.echo("‚úÖ Dependencies installed successfully")
        click.echo("üí° Use 'poetry shell' to activate the virtual environment")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Failed to install dependencies: {e}")
        click.echo("üí° Make sure Poetry is installed: https://python-poetry.org/docs/#installation")


@dev.command()
def setup():
    """Setup environment and configuration."""
    click.echo("‚öôÔ∏è  Setting up environment...")
    
    # Copy environment file
    env_example = Path("env.example")
    env_file = Path(".env")
    
    if env_example.exists() and not env_file.exists():
        import shutil
        shutil.copy(env_example, env_file)
        click.echo("‚úÖ Environment file created (.env)")
        click.echo("üìù Please edit .env with your configuration")
    else:
        click.echo("‚ÑπÔ∏è  Environment file already exists")


@dev.command()
def test():
    """Run tests using Poetry."""
    click.echo("üß™ Running tests...")
    try:
        subprocess.run(["poetry", "run", "pytest"], check=True)
        click.echo("‚úÖ Tests completed successfully")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Tests failed: {e}")


@dev.command()
def lint():
    """Run linting checks using Poetry."""
    click.echo("üîç Running linting checks...")
    try:
        subprocess.run(["poetry", "run", "flake8", ".", "--count", "--exit-zero", "--max-complexity=10", "--max-line-length=127"], check=True)
        click.echo("‚úÖ Linting passed")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Linting failed: {e}")


@dev.command()
def format():
    """Format code with black using Poetry."""
    click.echo("üé® Formatting code...")
    try:
        subprocess.run(["poetry", "run", "black", ".", "--line-length=127"], check=True)
        click.echo("‚úÖ Code formatted successfully")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Formatting failed: {e}")


@dev.command()
def clean():
    """Clean up temporary files and containers."""
    click.echo("üßπ Cleaning up...")
    
    try:
        # Stop containers
        subprocess.run(["docker", "compose", "-f", "infra/docker-compose.yml", "down", "-v"], check=True)
        click.echo("‚úÖ Containers stopped")
        
        # Clean Docker
        subprocess.run(["docker", "system", "prune", "-f"], check=True)
        click.echo("‚úÖ Docker cleaned")
        
        # Clean Python cache
        subprocess.run(["find", ".", "-type", "d", "-name", "__pycache__, -exec", "rm", "-rf", "{}", "+"], check=True)
        subprocess.run(["find", ".", "-type", "f", "-name", "*.pyc", "-delete"], check=True)
        click.echo("‚úÖ Python cache cleaned")
        
        # Remove checkpoints
        import shutil
        if Path(".checkpoints").exists():
            shutil.rmtree(".checkpoints")
            click.echo("‚úÖ Checkpoints cleaned")
        
        click.echo("üéâ Cleanup completed")
        
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Cleanup failed: {e}")


# ============================================================================
# Demo Command
# ============================================================================

@cli.command()
@click.option('--skip-infra', is_flag=True, help='Skip infrastructure checks')
@click.option('--quick', is_flag=True, help='Run quick demo with limited data')
@click.option('--continuous', is_flag=True, help='Run continuous pipeline mode')
def demo(skip_infra, quick, continuous):
    """Run a complete demonstration of the Breadth/Thrust Signals system."""
    click.echo("üéØ Breadth/Thrust Signals POC Demo")
    click.echo("=" * 50)
    
    # Check infrastructure
    if not skip_infra:
        click.echo("üîß Checking infrastructure...")
        health = check_infrastructure_health()
        
        if not health["all_healthy"]:
            click.echo("‚ùå Infrastructure not ready. Starting services...")
            subprocess.run(["docker-compose", "-f", "infra/docker-compose.yml", "up", "-d"])
            time.sleep(30)
            health = check_infrastructure_health()
            
            if not health["all_healthy"]:
                click.echo("‚ùå Failed to start infrastructure")
                click.echo("üí° Try: poetry run bf infra start")
                return
        
        click.echo("‚úÖ Infrastructure ready")
    else:
        click.echo("‚è≠Ô∏è  Skipping infrastructure checks")
    
    # Demo configuration
    if quick:
        symbol_list = "demo_small"
        start_date = "2024-06-01"
        end_date = "2024-12-31"
        initial_capital = 50000
        click.echo("‚ö° Running quick demo with limited data")
    else:
        symbol_list = "demo_medium"
        start_date = "2024-01-01"
        end_date = "2024-12-31"
        initial_capital = 100000
        click.echo("üöÄ Running full demo")
    
    if continuous:
        click.echo("üîÑ Running in continuous pipeline mode")
        click.echo("üìä Pipeline will run continuously until stopped")
        click.echo("‚èπÔ∏è  Press Ctrl+C to stop the pipeline")
    
    try:
        # Step 1: Data Summary
        click.echo("\nüìä Step 1: Data Summary")
        click.echo("-" * 30)
        subprocess.run(["poetry", "run", "bf", "data", "summary"], check=True)
        
        # Step 2: Fetch Sample Data
        click.echo("\nüì• Step 2: Fetch Sample Data")
        click.echo("-" * 30)
        subprocess.run([
            "poetry", "run", "bf", "data", "fetch",
            "--symbol-list", symbol_list,
            "--start-date", start_date,
            "--end-date", end_date
        ], check=True)
        
        # Step 3: Generate Signals
        click.echo("\nüéØ Step 3: Generate Signals")
        click.echo("-" * 30)
        subprocess.run([
            "poetry", "run", "bf", "signals", "generate",
            "--start-date", start_date,
            "--end-date", end_date,
            "--symbol-list", symbol_list
        ], check=True)
        
        # Step 4: Signal Summary
        click.echo("\nüìä Step 4: Signal Summary")
        click.echo("-" * 30)
        subprocess.run(["poetry", "run", "bf", "signals", "summary"], check=True)
        
        # Step 5: Run Backtest
        click.echo("\nüìà Step 5: Run Backtest")
        click.echo("-" * 30)
        subprocess.run([
            "poetry", "run", "bf", "backtest", "run",
            "--from-date", start_date,
            "--to-date", end_date,
            "--symbol-list", symbol_list,
            "--initial-capital", str(initial_capital),
            "--save-results"
        ], check=True)
        
        # Step 6: Analyze Results
        click.echo("\nüìä Step 6: Analyze Results")
        click.echo("-" * 30)
        subprocess.run(["poetry", "run", "bf", "backtest", "analyze"], check=True)
        
        # Demo completion
        click.echo("\nüéâ Demo completed successfully!")
        click.echo("=" * 50)
        
        # Show next steps
        click.echo("üí° Next Steps:")
        click.echo("   ‚Ä¢ Check web interfaces for detailed analysis")
        click.echo("   ‚Ä¢ Explore different time periods and symbols")
        click.echo("   ‚Ä¢ Adjust backtest parameters for optimization")
        click.echo("   ‚Ä¢ Run individual commands for specific analysis")
        
        # Show service URLs
        click.echo("\nüåê Web Interfaces:")
        show_service_urls()
        
        # Show available commands
        click.echo("\nüîß Available Commands:")
        click.echo("   ‚Ä¢ poetry run bf data fetch --help")
        click.echo("   ‚Ä¢ poetry run bf signals generate --help")
        click.echo("   ‚Ä¢ poetry run bf backtest run --help")
        click.echo("   ‚Ä¢ poetry run bf --help")
        
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Demo step failed: {e}")
        click.echo("üí° Check the error message above and try running individual commands")
    except Exception as e:
        click.echo(f"‚ùå Demo failed: {e}")
        click.echo("üí° Make sure all dependencies are installed: poetry install")


@cli.command()
@click.option('--symbol-list', default='demo_small', help='Symbol list to use')
@click.option('--interval', default=300, help='Interval between runs in seconds (default: 5 minutes)')
@click.option('--start-date', default='2024-01-01', help='Start date for analysis')
@click.option('--end-date', default='2024-12-31', help='End date for analysis')
@click.option('--auto-start-infra', is_flag=True, help='Automatically start infrastructure')
def pipeline(symbol_list, interval, start_date, end_date, auto_start_infra):
    """Run continuous pipeline mode - fetches data, generates signals, and runs backtests continuously."""
    click.echo("üîÑ Starting Continuous Pipeline Mode")
    click.echo("=" * 50)
    click.echo(f"üìä Symbol List: {symbol_list}")
    click.echo(f"‚è∞ Interval: {interval} seconds ({interval/60:.1f} minutes)")
    click.echo(f"üìÖ Date Range: {start_date} to {end_date}")
    click.echo(f"üèóÔ∏è  Auto-start Infrastructure: {auto_start_infra}")
    click.echo()
    click.echo("üîÑ Pipeline will run continuously until stopped")
    click.echo("‚èπÔ∏è  Press Ctrl+C to stop the pipeline")
    click.echo()
    
    # Start infrastructure if requested
    if auto_start_infra:
        click.echo("üöÄ Starting infrastructure...")
        try:
            subprocess.run(
                ["docker", "compose", "-f", "infra/docker-compose.yml", "up", "-d"],
                check=True
            )
            click.echo("‚úÖ Infrastructure started")
            time.sleep(30)  # Wait for services to be ready
        except subprocess.CalledProcessError as e:
            click.echo(f"‚ùå Failed to start infrastructure: {e}")
            return
    
    # Check infrastructure health
    health_status = check_infrastructure_health()
    if not health_status['all_healthy']:
        click.echo("‚ùå Infrastructure not healthy. Please start it first:")
        click.echo("   poetry run bf infra start")
        return
    
    click.echo("‚úÖ Infrastructure is healthy")
    
    run_count = 0
    start_time = datetime.now()
    
    try:
        while True:
            run_count += 1
            run_start = datetime.now()
            
            click.echo(f"\nüîÑ Pipeline Run #{run_count}")
            click.echo(f"‚è∞ Started at: {run_start.strftime('%Y-%m-%d %H:%M:%S')}")
            click.echo("-" * 40)
            
            try:
                # Step 1: Fetch Data
                click.echo("üì• Step 1: Fetching data...")
                fetch_result = subprocess.run([
                    "poetry", "run", "bf", "data", "fetch",
                    "--symbol-list", symbol_list,
                    "--start-date", start_date,
                    "--end-date", end_date
                ], capture_output=True, text=True, check=True)
                click.echo("‚úÖ Data fetched successfully")
                
                # Step 2: Generate Signals
                click.echo("üéØ Step 2: Generating signals...")
                signal_result = subprocess.run([
                    "poetry", "run", "bf", "signals", "generate",
                    "--symbol-list", symbol_list,
                    "--start-date", start_date,
                    "--end-date", end_date
                ], capture_output=True, text=True, check=True)
                click.echo("‚úÖ Signals generated successfully")
                
                # Step 3: Run Backtest
                click.echo("üìà Step 3: Running backtest...")
                backtest_result = subprocess.run([
                    "poetry", "run", "bf", "backtest", "run",
                    "--symbol-list", symbol_list,
                    "--from-date", start_date,
                    "--to-date", end_date,
                    "--save-results"
                ], capture_output=True, text=True, check=True)
                click.echo("‚úÖ Backtest completed successfully")
                
                # Calculate run time
                run_end = datetime.now()
                run_duration = (run_end - run_start).total_seconds()
                
                click.echo(f"‚úÖ Pipeline run #{run_count} completed in {run_duration:.1f}s")
                
                # Show summary
                total_duration = (datetime.now() - start_time).total_seconds()
                avg_duration = total_duration / run_count
                click.echo(f"üìä Total runs: {run_count}, Avg duration: {avg_duration:.1f}s")
                
                # Wait for next run
                if interval > 0:
                    click.echo(f"‚è≥ Waiting {interval} seconds until next run...")
                    click.echo(f"üïê Next run at: {(datetime.now() + timedelta(seconds=interval)).strftime('%Y-%m-%d %H:%M:%S')}")
                    time.sleep(interval)
                else:
                    click.echo("üîÑ Running immediately (no interval)")
                
            except subprocess.CalledProcessError as e:
                click.echo(f"‚ùå Pipeline run #{run_count} failed: {e}")
                click.echo(f"üìÑ Error output: {e.stderr}")
                click.echo("üîÑ Continuing with next run...")
                time.sleep(interval)
            except Exception as e:
                click.echo(f"‚ùå Unexpected error in run #{run_count}: {e}")
                click.echo("üîÑ Continuing with next run...")
                time.sleep(interval)
                
    except KeyboardInterrupt:
        click.echo("\n‚èπÔ∏è  Pipeline stopped by user")
        click.echo(f"üìä Total runs completed: {run_count}")
        click.echo(f"‚è±Ô∏è  Total time: {(datetime.now() - start_time).total_seconds():.1f}s")
        
        if auto_start_infra:
            click.echo("üõë Stopping infrastructure...")
            try:
                subprocess.run(
                    ["docker", "compose", "-f", "infra/docker-compose.yml", "down"],
                    check=True
                )
                click.echo("‚úÖ Infrastructure stopped")
            except subprocess.CalledProcessError as e:
                click.echo(f"‚ùå Failed to stop infrastructure: {e}")


@cli.command()
@click.option('--symbol-list', default='demo_small', help='Symbol list to use')
@click.option('--speed', default=60, help='Replay speed multiplier (default: 60x)')
@click.option('--auto-start-infra', is_flag=True, help='Automatically start infrastructure')
def stream(symbol_list, speed, auto_start_infra):
    """Run streaming mode - continuously replays data and generates real-time signals."""
    click.echo("üåä Starting Streaming Mode")
    click.echo("=" * 50)
    click.echo(f"üìä Symbol List: {symbol_list}")
    click.echo(f"‚ö° Speed: {speed}x (real-time)")
    click.echo(f"üèóÔ∏è  Auto-start Infrastructure: {auto_start_infra}")
    click.echo()
    click.echo("üåä Streaming will run continuously until stopped")
    click.echo("‚èπÔ∏è  Press Ctrl+C to stop the stream")
    click.echo()
    
    # Start infrastructure if requested
    if auto_start_infra:
        click.echo("üöÄ Starting infrastructure...")
        try:
            subprocess.run(
                ["docker", "compose", "-f", "infra/docker-compose.yml", "up", "-d"],
                check=True
            )
            click.echo("‚úÖ Infrastructure started")
            time.sleep(30)  # Wait for services to be ready
        except subprocess.CalledProcessError as e:
            click.echo(f"‚ùå Failed to start infrastructure: {e}")
            return
    
    # Check infrastructure health
    health_status = check_infrastructure_health()
    if not health_status['all_healthy']:
        click.echo("‚ùå Infrastructure not healthy. Please start it first:")
        click.echo("   poetry run bf infra start")
        return
    
    click.echo("‚úÖ Infrastructure is healthy")
    
    try:
        # Start data replay in background
        click.echo("üì° Starting data replay...")
        replay_process = subprocess.Popen([
            "poetry", "run", "bf", "data", "replay",
            "--symbol-list", symbol_list,
            "--speed", str(speed)
        ])
        
        click.echo("‚úÖ Data replay started")
        click.echo("üåä Streaming mode active - data is flowing through the system")
        click.echo("üìä Check web interfaces for real-time monitoring:")
        show_service_urls()
        
        # Keep running until interrupted
        try:
            while True:
                time.sleep(10)  # Check every 10 seconds
                if replay_process.poll() is not None:
                    click.echo("‚ùå Data replay process stopped unexpectedly")
                    break
        except KeyboardInterrupt:
            click.echo("\n‚èπÔ∏è  Stopping stream...")
            
    except Exception as e:
        click.echo(f"‚ùå Error in streaming mode: {e}")
    finally:
        # Clean up
        if 'replay_process' in locals():
            replay_process.terminate()
            click.echo("‚úÖ Data replay stopped")
        
        if auto_start_infra:
            click.echo("üõë Stopping infrastructure...")
            try:
                subprocess.run(
                    ["docker", "compose", "-f", "infra/docker-compose.yml", "down"],
                    check=True
                )
                click.echo("‚úÖ Infrastructure stopped")
            except subprocess.CalledProcessError as e:
                click.echo(f"‚ùå Failed to stop infrastructure: {e}")


@cli.command()
@click.option('--symbol-list', default='demo_small', help='Symbol list to use')
@click.option('--interval', default=300, help='Interval between runs in seconds')
def enhanced(symbol_list, interval):
    """Run enhanced pipeline with Airflow-like features (task history, retries)."""
    click.echo("üîÑ Enhanced Pipeline Mode")
    click.echo("=" * 50)
    click.echo(f"üìä Symbol List: {symbol_list}")
    click.echo(f"‚è∞ Interval: {interval} seconds")
    click.echo()
    click.echo("üîÑ Pipeline will run continuously until stopped")
    click.echo("‚èπÔ∏è  Press Ctrl+C to stop the pipeline")
    click.echo()
    
    # Import and run enhanced pipeline
    try:
        from cli.enhanced_pipeline import enhanced_pipeline
        enhanced_pipeline.callback(symbol_list, interval)  # Fixed: only pass 2 arguments
    except ImportError:
        click.echo("‚ùå Enhanced pipeline not available. Please ensure cli/enhanced_pipeline.py exists.")
    except Exception as e:
        click.echo(f"‚ùå Error running enhanced pipeline: {e}")


@cli.command()
@click.option('--port', default=8081, help='Dashboard port (default: 8081)')
@click.option('--host', default='localhost', help='Dashboard host (default: localhost)')
def dashboard(port, host):
    """Start the pipeline dashboard web interface."""
    click.echo("üåê Starting Pipeline Dashboard")
    click.echo("=" * 40)
    click.echo(f"üìç URL: http://{host}:{port}")
    click.echo(f"üìä Features: Pipeline history, task monitoring, run statistics")
    click.echo()
    click.echo("üåê Dashboard will run until stopped")
    click.echo("‚èπÔ∏è  Press Ctrl+C to stop the dashboard")
    click.echo()
    
    try:
        from cli.enhanced_pipeline import start_dashboard_server
        start_dashboard_server(host, port)
    except ImportError:
        click.echo("‚ùå Dashboard not available. Please ensure cli/enhanced_pipeline.py exists.")
    except Exception as e:
        click.echo(f"‚ùå Error starting dashboard: {e}")


# ============================================================================
# Utility Functions
# ============================================================================

def check_infrastructure_health() -> dict:
    """Check health of all infrastructure services."""
    services = [
        ("http://localhost:8080", "Spark UI"),
        ("http://localhost:9000/minio/health/live", "MinIO"),
        ("http://localhost:9200/_cluster/health", "Elasticsearch"),
        ("http://localhost:5601/api/status", "Kibana"),
    ]
    
    results = {}
    healthy_count = 0
    
    for url, name in services:
        try:
            response = requests.get(url, timeout=5)
            is_healthy = response.status_code == 200
            results[name] = is_healthy
            if is_healthy:
                healthy_count += 1
        except requests.exceptions.RequestException:
            results[name] = False
    
    return {
        'services': results,
        'healthy_count': healthy_count,
        'total_count': len(services),
        'all_healthy': healthy_count == len(services)
    }


def show_service_urls():
    """Show service URLs."""
    click.echo("\nüìã Service URLs:")
    click.echo("  ‚Ä¢ Spark UI: http://localhost:8080")
    click.echo("  ‚Ä¢ MinIO Console: http://localhost:9001 (minioadmin/minioadmin)")
    click.echo("  ‚Ä¢ Kibana: http://localhost:5601")
    click.echo("  ‚Ä¢ Elasticsearch: http://localhost:9200")
    click.echo("  ‚Ä¢ Kafka: localhost:9092")


if __name__ == '__main__':
    cli()
