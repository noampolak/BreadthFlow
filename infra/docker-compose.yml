version: '3.8'

services:
  # Spark Master
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - COMMAND_SERVER_PORT=8081
    ports:
      - "8080:8080"  # Spark UI
      - "7077:7077"  # Spark master port
      - "8081:8081"  # Command server port
    volumes:
      - spark-data:/opt/bitnami/spark
      - ../ingestion:/opt/bitnami/spark/jobs/ingestion:ro
      - ../features:/opt/bitnami/spark/jobs/features:ro
      - ../model:/opt/bitnami/spark/jobs/model:ro
      - ../backtests:/opt/bitnami/spark/jobs/backtests:ro
      - ../cli:/opt/bitnami/spark/jobs/cli:ro
      - spark-logs:/opt/bitnami/spark/logs
    healthcheck:
      test: ["CMD", "/opt/bitnami/spark/bin/spark-submit", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    command: >
      sh -c "
        python3 /opt/bitnami/spark/jobs/cli/spark_command_server.py &
        /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh
      "

  # Spark Worker 1
  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_CLEANUP_ENABLED=true
      - SPARK_WORKER_CLEANUP_INTERVAL=1800
      - SPARK_WORKER_CLEANUP_TTL=86400
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark-data:/opt/bitnami/spark
      - ../ingestion:/opt/bitnami/spark/jobs/ingestion:ro
      - ../features:/opt/bitnami/spark/jobs/features:ro
      - ../model:/opt/bitnami/spark/jobs/model:ro
      - ../backtests:/opt/bitnami/spark/jobs/backtests:ro
      - spark-logs:/opt/bitnami/spark/logs
    restart: unless-stopped

  # Spark Worker 2
  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_CLEANUP_ENABLED=true
      - SPARK_WORKER_CLEANUP_INTERVAL=1800
      - SPARK_WORKER_CLEANUP_TTL=86400
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark-data:/opt/bitnami/spark
      - ../ingestion:/opt/bitnami/spark/jobs/ingestion:ro
      - ../features:/opt/bitnami/spark/jobs/features:ro
      - ../model:/opt/bitnami/spark/jobs/model:ro
      - ../backtests:/opt/bitnami/spark/jobs/backtests:ro
      - ../cli:/opt/bitnami/spark/jobs/cli:ro
      - spark-logs:/opt/bitnami/spark/logs
    restart: unless-stopped

  # Spark History Server
  spark-history-server:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-history
    environment:
      - SPARK_MODE=history
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    volumes:
      - spark-logs:/opt/bitnami/spark/logs
    ports:
      - "18080:18080"
    restart: unless-stopped

  # Apache Kafka
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:9094,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_DELETE_TOPIC_ENABLE=true
      - KAFKA_CFG_LOG_RETENTION_HOURS=168
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS=300000
    ports:
      - "9092:9092"
      - "9094:9094"
    volumes:
      - kafka-data:/bitnami/kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kafdrop - Kafka UI
  kafdrop:
    image: obsidiandynamics/kafdrop:4.0.0
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9002:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:9092
      JVM_OPTS: "-Xms32M -Xmx64M"
      SERVER_SERVLET_CONTEXTPATH: "/"
    restart: unless-stopped

  # MinIO Object Storage (S3-compatible)
  minio:
    image: minio/minio:RELEASE.2024-05-10T01-41-38Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_DOMAIN: localhost
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # MinIO Console
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD-SHELL", "timeout 10 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/9000'"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # PostgreSQL Database for Pipeline Metadata
  postgres:
    image: postgres:15-alpine
    container_name: breadthflow-postgres
    environment:
      POSTGRES_DB: breadthflow
      POSTGRES_USER: pipeline
      POSTGRES_PASSWORD: pipeline123
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U pipeline -d breadthflow"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # BreadthFlow Web Dashboard (Legacy)
  dashboard:
    build:
      context: ..
      dockerfile: infra/Dockerfile.dashboard
    container_name: breadthflow-dashboard
    environment:
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - DATABASE_URL=postgresql://pipeline:pipeline123@postgres:5432/breadthflow
    ports:
      - "8083:8080"  # Dashboard on port 8083
    volumes:
      - ../cli:/app/cli:ro
    depends_on:
      - postgres
      - elasticsearch
      - minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # FastAPI Backend (New)
  breadthflow-api:
    build:
      context: ../fastapi_app
      dockerfile: Dockerfile
    container_name: breadthflow-api
    ports:
      - "8005:8005"
    environment:
      - DATABASE_URL=postgresql://pipeline:pipeline123@postgres:5432/breadthflow
      - REDIS_URL=redis://redis:6379
      - SPARK_COMMAND_SERVER_URL=http://spark-master:8081
      - DEBUG=true
    depends_on:
      - postgres
      - redis
    volumes:
      - ../fastapi_app:/app
    restart: unless-stopped

  # React Frontend (New)
  breadthflow-frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    container_name: breadthflow-frontend
    ports:
      - "3005:3005"
    environment:
      - REACT_APP_API_URL=http://localhost:8005
      - REACT_APP_WS_URL=ws://localhost:8005/ws
    volumes:
      - ../frontend:/app
      - /app/node_modules
    restart: unless-stopped

  # Redis for caching and WebSocket
  redis:
    image: redis:7-alpine
    container_name: breadthflow-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # Zookeeper (for Kafka)
  zookeeper:
    image: bitnami/zookeeper:3.9
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/bitnami/zookeeper
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  spark-data:
  spark-logs:
  kafka-data:
  minio-data:
  elasticsearch-data:
  zookeeper-data:
  dashboard-data:
  postgres-data:
  redis_data:

networks:
  default:
    name: breadthflow-network
