version: '3.8'

services:
  # Spark Master
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    ports:
      - "8080:8080"  # Spark UI
      - "7077:7077"  # Spark master port
    volumes:
      - spark-data:/opt/bitnami/spark
      - ../ingestion:/opt/bitnami/spark/jobs/ingestion:ro
      - ../features:/opt/bitnami/spark/jobs/features:ro
      - ../model:/opt/bitnami/spark/jobs/model:ro
      - ../backtests:/opt/bitnami/spark/jobs/backtests:ro
      - ../cli:/opt/bitnami/spark/jobs/cli:ro
      - spark-logs:/opt/bitnami/spark/logs
    healthcheck:
      test: ["CMD", "/opt/bitnami/spark/bin/spark-submit", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Spark Worker 1
  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_CLEANUP_ENABLED=true
      - SPARK_WORKER_CLEANUP_INTERVAL=1800
      - SPARK_WORKER_CLEANUP_TTL=86400
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark-data:/opt/bitnami/spark
      - ../ingestion:/opt/bitnami/spark/jobs/ingestion:ro
      - ../features:/opt/bitnami/spark/jobs/features:ro
      - ../model:/opt/bitnami/spark/jobs/model:ro
      - ../backtests:/opt/bitnami/spark/jobs/backtests:ro
      - spark-logs:/opt/bitnami/spark/logs
    restart: unless-stopped

  # Spark Worker 2
  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_CLEANUP_ENABLED=true
      - SPARK_WORKER_CLEANUP_INTERVAL=1800
      - SPARK_WORKER_CLEANUP_TTL=86400
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark-data:/opt/bitnami/spark
      - ../ingestion:/opt/bitnami/spark/jobs/ingestion:ro
      - ../features:/opt/bitnami/spark/jobs/features:ro
      - ../model:/opt/bitnami/spark/jobs/model:ro
      - ../backtests:/opt/bitnami/spark/jobs/backtests:ro
      - ../cli:/opt/bitnami/spark/jobs/cli:ro
      - spark-logs:/opt/bitnami/spark/logs
    restart: unless-stopped

  # Spark History Server
  spark-history-server:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-history
    environment:
      - SPARK_MODE=history
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    volumes:
      - spark-logs:/opt/bitnami/spark/logs
    ports:
      - "18080:18080"
    restart: unless-stopped

  # Apache Kafka
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_DELETE_TOPIC_ENABLE=true
      - KAFKA_CFG_LOG_RETENTION_HOURS=168
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS=300000
    ports:
      - "9092:9092"
    volumes:
      - kafka-data:/bitnami/kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # MinIO Object Storage (S3-compatible)
  minio:
    image: minio/minio:RELEASE.2024-05-10T01-41-38Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_DOMAIN: localhost
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # MinIO Console
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Zookeeper (for Kafka)
  zookeeper:
    image: bitnami/zookeeper:3.9
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/bitnami/zookeeper
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  spark-data:
  spark-logs:
  kafka-data:
  minio-data:
  elasticsearch-data:
  zookeeper-data:

networks:
  default:
    name: breadthflow-network
