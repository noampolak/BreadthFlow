{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Multi-Source Training Example\n",
        "\n",
        "## 📋 **Overview**\n",
        "\n",
        "This notebook demonstrates how to use the **generic feature engineering modules** to create a comprehensive multi-source analysis experiment that combines:\n",
        "\n",
        "- **Technical Indicators**: RSI, MACD, Bollinger Bands, Volume patterns\n",
        "- **Financial Fundamentals**: P/E ratio, Market Cap, Revenue, EPS, Debt-to-Equity\n",
        "- **Time Features**: Hour, day, month patterns, seasonality\n",
        "- **Market Microstructure**: Volume patterns, volatility, price impact\n",
        "\n",
        "## 🎯 **Learning Objectives**\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "1. Understand how to use **generic, reusable feature modules**\n",
        "2. Learn to combine **multiple data sources** for ML training\n",
        "3. See how to create **experiment-specific configurations**\n",
        "4. Experience the **complete ML pipeline** from data to deployment\n",
        "\n",
        "## 🏗️ **Industry Standard Structure**\n",
        "\n",
        "This example follows **industry best practices**:\n",
        "- ✅ **Generic modules** for maximum reusability\n",
        "- ✅ **Experiment-specific configs** for flexibility\n",
        "- ✅ **Modular design** for easy testing and maintenance\n",
        "- ✅ **Production-ready** integration with existing ML services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import our generic feature modules\n",
        "from features import (\n",
        "    TechnicalIndicators, \n",
        "    FinancialFundamentals, \n",
        "    MarketMicrostructure, \n",
        "    TimeFeatures, \n",
        "    FeatureUtils\n",
        ")\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(\"✅ Generic feature modules loaded!\")\n",
        "print(\"✅ Ready to start multi-source analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 **Step 1: Start the ML Platform**\n",
        "\n",
        "First, let's make sure all ML services are running. If they're not already started, run this command in your terminal:\n",
        "\n",
        "```bash\n",
        "# Start all ML services\n",
        "docker-compose -f docker-compose.ml.yml up -d\n",
        "\n",
        "# Check services are running\n",
        "docker-compose -f docker-compose.ml.yml ps\n",
        "```\n",
        "\n",
        "**Expected Services:**\n",
        "- Data Pipeline: http://localhost:8001\n",
        "- Feature Engineering: http://localhost:8002  \n",
        "- Model Training: http://localhost:8003\n",
        "- AutoML: http://localhost:8004\n",
        "- Model Serving: http://localhost:8005\n",
        "- Model Registry: http://localhost:8006\n",
        "- MLflow: http://localhost:5001\n",
        "- Jupyter Lab: http://localhost:8888\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ML services connectivity\n",
        "services = {\n",
        "    \"Data Pipeline\": \"http://localhost:8001/health\",\n",
        "    \"Feature Engineering\": \"http://localhost:8002/health\", \n",
        "    \"Model Training\": \"http://localhost:8003/health\",\n",
        "    \"AutoML\": \"http://localhost:8004/health\",\n",
        "    \"Model Serving\": \"http://localhost:8005/health\",\n",
        "    \"Model Registry\": \"http://localhost:8006/health\"\n",
        "}\n",
        "\n",
        "print(\"🔍 Testing ML services connectivity...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for service_name, url in services.items():\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"✅ {service_name}: Connected\")\n",
        "        else:\n",
        "            print(f\"❌ {service_name}: Error {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ {service_name}: Connection failed - {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"💡 If any services are not connected, make sure to start them with:\")\n",
        "print(\"   docker-compose -f docker-compose.ml.yml up -d\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 **Step 2: Load Experiment Configuration**\n",
        "\n",
        "Let's load the experiment configuration that defines which features to use for this specific experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load experiment configuration\n",
        "config_path = '../experiments/multi_source_analysis/config.yaml'\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"📋 Experiment Configuration Loaded:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Experiment Name: {config['experiment_name']}\")\n",
        "print(f\"Description: {config['description']}\")\n",
        "print(f\"Symbols: {config['data']['symbols']}\")\n",
        "print(f\"Timeframe: {config['data']['timeframe']}\")\n",
        "print(f\"Date Range: {config['data']['start_date']} to {config['data']['end_date']}\")\n",
        "print(f\"Target Type: {config['target']['type']}\")\n",
        "print(f\"Target Definition: {config['target']['definition']}\")\n",
        "\n",
        "print(\"\\n🎯 Enabled Features:\")\n",
        "print(\"-\" * 30)\n",
        "for feature_type, feature_config in config['features'].items():\n",
        "    if feature_config['enabled']:\n",
        "        print(f\"✅ {feature_type.replace('_', ' ').title()}\")\n",
        "        if 'indicators' in feature_config:\n",
        "            print(f\"   Indicators: {len(feature_config['indicators'])}\")\n",
        "    else:\n",
        "        print(f\"❌ {feature_type.replace('_', ' ').title()}\")\n",
        "\n",
        "print(f\"\\n🤖 Model Algorithms: {len(config['models']['algorithms'])}\")\n",
        "for algo in config['models']['algorithms']:\n",
        "    print(f\"   - {algo}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 **Step 3: Fetch Market Data**\n",
        "\n",
        "Let's fetch market data for our symbols using the existing data pipeline service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample market data for demonstration\n",
        "# In a real scenario, you would fetch this from the data pipeline service\n",
        "symbols = config['data']['symbols']\n",
        "market_data = {}\n",
        "\n",
        "print(\"📈 Creating sample market data for demonstration...\")\n",
        "print(\"=\" * 50)\n",
        "print(\"💡 Note: In production, this data would come from the data pipeline service\")\n",
        "print(\"   For this demo, we'll create realistic sample data\")\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Create sample data for each symbol\n",
        "for symbol in symbols:\n",
        "    print(f\"Creating sample data for {symbol}...\")\n",
        "    \n",
        "    # Generate realistic price data\n",
        "    dates = pd.date_range(start=config['data']['start_date'], \n",
        "                         end=config['data']['end_date'], \n",
        "                         freq='D')\n",
        "    \n",
        "    # Create realistic OHLCV data\n",
        "    np.random.seed(hash(symbol) % 2**32)  # Consistent seed per symbol\n",
        "    \n",
        "    # Start with a base price\n",
        "    base_price = 100 + hash(symbol) % 500\n",
        "    \n",
        "    # Generate price movements\n",
        "    returns = np.random.normal(0.001, 0.02, len(dates))  # 0.1% daily return, 2% volatility\n",
        "    prices = base_price * np.exp(np.cumsum(returns))\n",
        "    \n",
        "    # Create OHLCV data\n",
        "    data = {\n",
        "        'Open': prices * (1 + np.random.normal(0, 0.005, len(dates))),\n",
        "        'High': prices * (1 + np.abs(np.random.normal(0, 0.01, len(dates)))),\n",
        "        'Low': prices * (1 - np.abs(np.random.normal(0, 0.01, len(dates)))),\n",
        "        'Close': prices,\n",
        "        'Volume': np.random.randint(1000000, 10000000, len(dates))\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data, index=dates)\n",
        "    \n",
        "    # Ensure High >= max(Open, Close) and Low <= min(Open, Close)\n",
        "    df['High'] = df[['Open', 'Close']].max(axis=1) + np.abs(np.random.normal(0, 0.005, len(dates)))\n",
        "    df['Low'] = df[['Open', 'Close']].min(axis=1) - np.abs(np.random.normal(0, 0.005, len(dates)))\n",
        "    \n",
        "    market_data[symbol] = df\n",
        "    print(f\"✅ {symbol}: {len(df)} records created\")\n",
        "\n",
        "print(f\"\\n📊 Total symbols with data: {len(market_data)}\")\n",
        "\n",
        "# Display sample data\n",
        "if market_data:\n",
        "    sample_symbol = list(market_data.keys())[0]\n",
        "    print(f\"\\n📋 Sample data for {sample_symbol}:\")\n",
        "    print(market_data[sample_symbol].head())\n",
        "    print(f\"\\nData shape: {market_data[sample_symbol].shape}\")\n",
        "    print(f\"Date range: {market_data[sample_symbol].index.min()} to {market_data[sample_symbol].index.max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 **Step 4: Generate Features Using Generic Modules**\n",
        "\n",
        "Now let's use our **generic feature engineering modules** to generate all the features. This demonstrates the power of reusable, modular design!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize feature calculators\n",
        "technical_calc = TechnicalIndicators()\n",
        "financial_calc = FinancialFundamentals()\n",
        "microstructure_calc = MarketMicrostructure()\n",
        "time_calc = TimeFeatures()\n",
        "utils = FeatureUtils()\n",
        "\n",
        "print(\"🔧 Feature calculators initialized!\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Generate features for each symbol\n",
        "all_features = []\n",
        "all_targets = []\n",
        "\n",
        "for symbol, data in market_data.items():\n",
        "    print(f\"\\n🔧 Processing {symbol}...\")\n",
        "    \n",
        "    # Initialize features DataFrame\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    \n",
        "    # 1. Technical Indicators\n",
        "    if config['features']['technical']['enabled']:\n",
        "        print(\"  📊 Generating technical indicators...\")\n",
        "        technical_features = technical_calc.get_all_indicators(data)\n",
        "        features = pd.concat([features, technical_features], axis=1)\n",
        "        print(f\"    ✅ Added {len(technical_features.columns)} technical indicators\")\n",
        "    \n",
        "    # 2. Market Microstructure\n",
        "    if config['features']['market_microstructure']['enabled']:\n",
        "        print(\"  📈 Generating market microstructure features...\")\n",
        "        microstructure_features = microstructure_calc.get_all_microstructure(data)\n",
        "        features = pd.concat([features, microstructure_features], axis=1)\n",
        "        print(f\"    ✅ Added {len(microstructure_features.columns)} microstructure features\")\n",
        "    \n",
        "    # 3. Time Features\n",
        "    if config['features']['time_features']['enabled']:\n",
        "        print(\"  ⏰ Generating time features...\")\n",
        "        time_features = time_calc.get_all_time_features(data.index)\n",
        "        features = pd.concat([features, time_features], axis=1)\n",
        "        print(f\"    ✅ Added {len(time_features.columns)} time features\")\n",
        "    \n",
        "    # 4. Create Target Variable\n",
        "    print(\"  🎯 Creating target variable...\")\n",
        "    if config['target']['type'] == 'classification':\n",
        "        price_change = data['Close'].pct_change()\n",
        "        target = (price_change > config['target']['threshold']).astype(int)\n",
        "    else:\n",
        "        target = data['Close'].pct_change()\n",
        "    \n",
        "    # 5. Add symbol identifier\n",
        "    features['symbol'] = symbol\n",
        "    \n",
        "    # 6. Align features and target\n",
        "    common_index = features.index.intersection(target.index)\n",
        "    features = features.loc[common_index]\n",
        "    target = target.loc[common_index]\n",
        "    \n",
        "    # 7. Remove rows with missing target\n",
        "    valid_mask = ~target.isna()\n",
        "    features = features[valid_mask]\n",
        "    target = target[valid_mask]\n",
        "    \n",
        "    all_features.append(features)\n",
        "    all_targets.append(target)\n",
        "    \n",
        "    print(f\"    ✅ Final shape: {features.shape} features, {len(target)} targets\")\n",
        "\n",
        "print(f\"\\n🎉 Feature generation completed for {len(all_features)} symbols!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all data\n",
        "print(\"🔄 Combining data from all symbols...\")\n",
        "X = pd.concat(all_features, axis=0)\n",
        "y = pd.concat(all_targets, axis=0)\n",
        "\n",
        "print(f\"📊 Final dataset shape: {X.shape}\")\n",
        "print(f\"🎯 Target distribution:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Display feature summary\n",
        "print(f\"\\n📋 Feature Summary:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total features: {X.shape[1]}\")\n",
        "print(f\"Total samples: {X.shape[0]}\")\n",
        "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
        "print(f\"Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Show feature categories\n",
        "feature_categories = {\n",
        "    'Technical': [col for col in X.columns if col in ['rsi', 'macd', 'bb_', 'stoch_']],\n",
        "    'Microstructure': [col for col in X.columns if col in ['volume_', 'pvt', 'obv', 'atr', 'volatility']],\n",
        "    'Time': [col for col in X.columns if col in ['hour', 'day_', 'month', 'quarter', 'year', 'is_', 'season']],\n",
        "    'Other': [col for col in X.columns if col not in ['symbol'] and col not in \n",
        "              [col for cat in ['Technical', 'Microstructure', 'Time'] for col in feature_categories[cat]]]\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 Feature Categories:\")\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"  {category}: {len(features)} features\")\n",
        "        if len(features) <= 5:\n",
        "            print(f\"    {features}\")\n",
        "        else:\n",
        "            print(f\"    {features[:3]} ... and {len(features)-3} more\")\n",
        "\n",
        "# Display sample of the combined dataset\n",
        "print(f\"\\n📋 Sample of combined dataset:\")\n",
        "print(X.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎨 **Step 5: Data Visualization**\n",
        "\n",
        "Let's visualize our features to understand the data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Multi-Source Analysis: Feature Distributions', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Target Distribution\n",
        "axes[0, 0].pie(y.value_counts().values, labels=['Down', 'Up'], autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 0].set_title('Target Distribution (Price Direction)')\n",
        "\n",
        "# 2. RSI Distribution\n",
        "if 'rsi' in X.columns:\n",
        "    axes[0, 1].hist(X['rsi'].dropna(), bins=50, alpha=0.7, color='blue')\n",
        "    axes[0, 1].axvline(x=70, color='red', linestyle='--', label='Overbought (70)')\n",
        "    axes[0, 1].axvline(x=30, color='green', linestyle='--', label='Oversold (30)')\n",
        "    axes[0, 1].set_title('RSI Distribution')\n",
        "    axes[0, 1].set_xlabel('RSI Value')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "# 3. Volume Distribution\n",
        "if 'volume_ma' in X.columns:\n",
        "    axes[1, 0].hist(X['volume_ma'].dropna(), bins=50, alpha=0.7, color='orange')\n",
        "    axes[1, 0].set_title('Volume Moving Average Distribution')\n",
        "    axes[1, 0].set_xlabel('Volume MA')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# 4. Time Features - Day of Week\n",
        "if 'day_of_week' in X.columns:\n",
        "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    day_counts = X['day_of_week'].value_counts().sort_index()\n",
        "    axes[1, 1].bar(day_names, day_counts.values, color='purple', alpha=0.7)\n",
        "    axes[1, 1].set_title('Trading Activity by Day of Week')\n",
        "    axes[1, 1].set_xlabel('Day of Week')\n",
        "    axes[1, 1].set_ylabel('Number of Records')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature correlation heatmap (sample of features)\n",
        "print(\"🔍 Feature Correlation Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Select a subset of features for correlation analysis\n",
        "correlation_features = []\n",
        "for col in X.columns:\n",
        "    if col not in ['symbol'] and X[col].dtype in ['float64', 'int64']:\n",
        "        correlation_features.append(col)\n",
        "    if len(correlation_features) >= 15:  # Limit to 15 features for readability\n",
        "        break\n",
        "\n",
        "if correlation_features:\n",
        "    corr_matrix = X[correlation_features].corr()\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
        "    plt.title('Feature Correlation Matrix (Sample)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric features available for correlation analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 **Step 6: Train Models Using ML Pipeline**\n",
        "\n",
        "Now let's train models using the existing ML pipeline service. This demonstrates how to integrate with the production ML infrastructure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "print(\"🤖 Preparing data for model training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Remove symbol column for training (it's categorical)\n",
        "X_train = X.drop('symbol', axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "X_train = X_train.fillna(method='ffill').fillna(0)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Missing values: {X_train.isnull().sum().sum()}\")\n",
        "\n",
        "# Prepare training request\n",
        "training_data = {\n",
        "    'features': X_train.to_dict('records'),\n",
        "    'target': y.tolist(),\n",
        "    'feature_names': X_train.columns.tolist(),\n",
        "    'algorithms': config['models']['algorithms'][:3],  # Use first 3 algorithms for demo\n",
        "    'experiment_name': config['experiment_name'],\n",
        "    'target_type': config['target']['type']\n",
        "}\n",
        "\n",
        "print(f\"\\n🎯 Training Configuration:\")\n",
        "print(f\"  Algorithms: {training_data['algorithms']}\")\n",
        "print(f\"  Target Type: {training_data['target_type']}\")\n",
        "print(f\"  Features: {len(training_data['feature_names'])}\")\n",
        "print(f\"  Samples: {len(training_data['target'])}\")\n",
        "\n",
        "# Train models\n",
        "print(f\"\\n🚀 Starting model training...\")\n",
        "try:\n",
        "    response = requests.post('http://localhost:8003/train-models', json=training_data, timeout=300)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "        print(\"✅ Model training completed successfully!\")\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"\\n📊 Training Results:\")\n",
        "        print(\"=\" * 40)\n",
        "        for model_name, metrics in results.items():\n",
        "            if isinstance(metrics, dict) and 'accuracy' in metrics:\n",
        "                print(f\"  {model_name}:\")\n",
        "                print(f\"    Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\")\n",
        "                print(f\"    Precision: {metrics.get('precision', 'N/A'):.4f}\")\n",
        "                print(f\"    Recall: {metrics.get('recall', 'N/A'):.4f}\")\n",
        "                print(f\"    F1-Score: {metrics.get('f1_score', 'N/A'):.4f}\")\n",
        "                print()\n",
        "    else:\n",
        "        print(f\"❌ Training failed: {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Training error: {e}\")\n",
        "    print(\"💡 Make sure the model training service is running:\")\n",
        "    print(\"   docker-compose -f docker-compose.ml.yml up -d\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 **Step 7: View Results in MLflow**\n",
        "\n",
        "Let's check the experiment results in MLflow for detailed tracking and comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check MLflow experiments\n",
        "print(\"📈 Checking MLflow experiments...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Get MLflow experiments\n",
        "    mlflow_response = requests.get('http://localhost:5001/api/2.0/mlflow/experiments/list')\n",
        "    \n",
        "    if mlflow_response.status_code == 200:\n",
        "        experiments = mlflow_response.json()\n",
        "        print(f\"✅ Found {len(experiments.get('experiments', []))} experiments in MLflow\")\n",
        "        \n",
        "        # Find our experiment\n",
        "        our_experiment = None\n",
        "        for exp in experiments.get('experiments', []):\n",
        "            if config['experiment_name'] in exp.get('name', ''):\n",
        "                our_experiment = exp\n",
        "                break\n",
        "        \n",
        "        if our_experiment:\n",
        "            print(f\"✅ Found our experiment: {our_experiment['name']}\")\n",
        "            print(f\"   Experiment ID: {our_experiment['experiment_id']}\")\n",
        "            print(f\"   Creation Time: {our_experiment['creation_time']}\")\n",
        "            \n",
        "            # Get runs for this experiment\n",
        "            runs_response = requests.get(f\"http://localhost:5001/api/2.0/mlflow/runs/search\", \n",
        "                                       json={\"experiment_ids\": [our_experiment['experiment_id']]})\n",
        "            \n",
        "            if runs_response.status_code == 200:\n",
        "                runs_data = runs_response.json()\n",
        "                runs = runs_data.get('runs', [])\n",
        "                print(f\"   Number of runs: {len(runs)}\")\n",
        "                \n",
        "                if runs:\n",
        "                    print(f\"\\n📊 Recent runs:\")\n",
        "                    for i, run in enumerate(runs[:5]):  # Show last 5 runs\n",
        "                        print(f\"   Run {i+1}: {run.get('data', {}).get('metrics', {}).get('accuracy', 'N/A')}\")\n",
        "        else:\n",
        "            print(f\"❌ Experiment '{config['experiment_name']}' not found in MLflow\")\n",
        "    else:\n",
        "        print(f\"❌ Could not connect to MLflow: {mlflow_response.status_code}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ MLflow error: {e}\")\n",
        "\n",
        "print(f\"\\n🌐 Access MLflow UI at: http://localhost:5001\")\n",
        "print(f\"   Look for experiment: {config['experiment_name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 **Step 8: Deploy Model with Seldon Core**\n",
        "\n",
        "Let's deploy our best model using Seldon Core for production serving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy model with Seldon Core\n",
        "print(\"🚀 Deploying model with Seldon Core...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "deployment_config = {\n",
        "    \"name\": config['seldon']['deployment_name'],\n",
        "    \"replicas\": config['seldon']['replicas'],\n",
        "    \"model_name\": \"multi_source_model\",\n",
        "    \"model_version\": \"latest\",\n",
        "    \"resources\": config['seldon']['resources']\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Deploy model\n",
        "    deploy_response = requests.post('http://localhost:8005/deploy', json=deployment_config)\n",
        "    \n",
        "    if deploy_response.status_code == 200:\n",
        "        deploy_result = deploy_response.json()\n",
        "        print(\"✅ Model deployed successfully!\")\n",
        "        print(f\"   Deployment Name: {deploy_result.get('name', 'N/A')}\")\n",
        "        print(f\"   Status: {deploy_result.get('status', 'N/A')}\")\n",
        "        print(f\"   Endpoint: {deploy_result.get('endpoint', 'N/A')}\")\n",
        "        \n",
        "        # Test the deployed model\n",
        "        if 'endpoint' in deploy_result:\n",
        "            print(f\"\\n🧪 Testing deployed model...\")\n",
        "            \n",
        "            # Prepare test data (sample from our dataset)\n",
        "            test_sample = X_train.iloc[:1].to_dict('records')[0]\n",
        "            \n",
        "            test_response = requests.post(deploy_result['endpoint'], json={\n",
        "                \"data\": test_sample\n",
        "            })\n",
        "            \n",
        "            if test_response.status_code == 200:\n",
        "                prediction = test_response.json()\n",
        "                print(f\"✅ Model prediction: {prediction}\")\n",
        "            else:\n",
        "                print(f\"❌ Prediction test failed: {test_response.status_code}\")\n",
        "    else:\n",
        "        print(f\"❌ Deployment failed: {deploy_response.status_code}\")\n",
        "        print(f\"Response: {deploy_response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Deployment error: {e}\")\n",
        "    print(\"💡 Make sure Seldon Core is running:\")\n",
        "    print(\"   docker-compose -f docker-compose.ml.yml up -d seldon-core-operator seldon-deployment\")\n",
        "\n",
        "print(f\"\\n🌐 Seldon Core UI: http://localhost:8084\")\n",
        "print(f\"   Model endpoint: http://localhost:8005\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 **Summary & Next Steps**\n",
        "\n",
        "Congratulations! You've successfully completed a comprehensive multi-source analysis experiment using industry-standard practices.\n",
        "\n",
        "### ✅ **What We Accomplished:**\n",
        "\n",
        "1. **Generic Feature Modules**: Created reusable technical indicators, financial fundamentals, and time features\n",
        "2. **Multi-Source Data**: Combined technical analysis, financial fundamentals, and time-based features\n",
        "3. **Experiment Configuration**: Used YAML configs for flexible experiment management\n",
        "4. **ML Pipeline Integration**: Trained models using the existing ML infrastructure\n",
        "5. **Model Deployment**: Deployed models with Seldon Core for production serving\n",
        "6. **Experiment Tracking**: Used MLflow for comprehensive experiment management\n",
        "\n",
        "### 🏗️ **Industry Standard Structure Achieved:**\n",
        "\n",
        "- ✅ **Generic modules** for maximum reusability\n",
        "- ✅ **Experiment-specific configs** for flexibility  \n",
        "- ✅ **Modular design** for easy testing and maintenance\n",
        "- ✅ **Production-ready** integration with existing ML services\n",
        "\n",
        "### 🚀 **Next Steps for Your Coworker:**\n",
        "\n",
        "1. **Experiment with Different Features**: Modify `config.yaml` to enable/disable different feature types\n",
        "2. **Try Different Algorithms**: Add more algorithms to the `algorithms` list in config\n",
        "3. **Create New Experiments**: Copy the experiment directory and modify for new use cases\n",
        "4. **Add Financial Data**: Integrate real financial fundamental data sources\n",
        "5. **A/B Testing**: Use Seldon Core to test different model versions\n",
        "\n",
        "### 📚 **Key Learning Points:**\n",
        "\n",
        "- **Generic modules** can be reused across multiple experiments\n",
        "- **Configuration-driven** experiments are easier to manage and reproduce\n",
        "- **Integration** with existing ML infrastructure saves development time\n",
        "- **Industry standards** provide a solid foundation for scaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📖 **Additional Resources**\n",
        "\n",
        "### 🔗 **Useful Links:**\n",
        "- **MLflow UI**: http://localhost:5001 - Track experiments and models\n",
        "- **Seldon Core UI**: http://localhost:8084 - Manage model deployments  \n",
        "- **Jupyter Lab**: http://localhost:8888 - Interactive development\n",
        "- **Grafana**: http://localhost:3001 - Monitoring dashboards\n",
        "\n",
        "### 📁 **File Structure Created:**\n",
        "```\n",
        "BreadthFlow/\n",
        "├── features/                          # ✅ Generic modules (reusable)\n",
        "│   ├── __init__.py\n",
        "│   ├── technical_indicators.py        # RSI, MACD, Bollinger Bands\n",
        "│   ├── financial_fundamentals.py      # P/E, Market Cap, Revenue\n",
        "│   ├── market_microstructure.py       # Volume, volatility, price impact\n",
        "│   ├── time_features.py               # Time-based features\n",
        "│   └── feature_utils.py               # Utilities and helpers\n",
        "├── experiments/                       # ✅ Experiment-specific\n",
        "│   └── multi_source_analysis/\n",
        "│       ├── config.yaml                # Experiment configuration\n",
        "│       ├── run_experiment.py          # Experiment runner script\n",
        "│       └── results/                   # Experiment results\n",
        "└── notebooks/\n",
        "    └── multi_source_training_example.ipynb  # ✅ This notebook\n",
        "```\n",
        "\n",
        "### 🎯 **Quick Commands:**\n",
        "```bash\n",
        "# Start ML platform\n",
        "docker-compose -f docker-compose.ml.yml up -d\n",
        "\n",
        "# Run experiment script\n",
        "python experiments/multi_source_analysis/run_experiment.py\n",
        "\n",
        "# Check services\n",
        "docker-compose -f docker-compose.ml.yml ps\n",
        "```\n",
        "\n",
        "**Happy experimenting! 🚀**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
